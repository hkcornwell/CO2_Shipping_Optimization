{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading, Cleansing and Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to AWS PostgreSQL Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that the team leveraged is AIS data procured through the federal government. The original dataset can be obtained through the following link: https://marinecadastre.gov/ais/\n",
    "\n",
    "In reviewing the data, the team wanted to review all of the data available. However, this dataset was incredibly large, and as the team learned all zone-data for one month represented was more than 50GB. As such, the team loaded all zones data for December 2017 in to a PostgreSQL instance on AWS to analyze and conduct our work. This dataset was a total of 64 GB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team first loaded the various Python packages that were considered necessary:\n",
    "* Pandas, Numpy and Math were imported to help with the data cleansing and data anlaysis. \n",
    "* SKLearn.Cluster was imported for the analytics algorithm utilized. \n",
    "* SQLAlchemy was imported due to its connectivity to cloud-hosted databases. SQLAlchemy also leverages psycopg2, which is a PostgreSQL database adapter for Python. \n",
    "* Matplotlib was imported to view some of the data as part of the analysis, to gain a better understanding of the data and what visualizations it would be capabale of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team leveraged the steps included in the following link to connect to a cloud-hosted PostgreSQL server: https://blog.panoply.io/connecting-jupyter-notebook-with-postgresql-for-python-data-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postgres username, password, and database name\n",
    "\n",
    "POSTGRES_ADDRESS = 'mydbinstance.cncrogettxf7.us-east-1.rds.amazonaws.com' ## INSERT YOUR DB ADDRESS IF IT'S NOT ON PANOPLY\n",
    "POSTGRES_PORT = '5432'\n",
    "POSTGRES_USERNAME = 'mydbinstance' ## CHANGE THIS TO YOUR PANOPLY/POSTGRES USERNAME\n",
    "POSTGRES_PASSWORD = 'FjhpW9aVvhM66r5Ux' ## CHANGE THIS TO YOUR PANOPLY/POSTGRES PASSWORD \n",
    "POSTGRES_DBNAME = 'postgres' ## CHANGE THIS TO YOUR DATABASE NAME\n",
    "# A long string that contains the necessary Postgres login information\n",
    "postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'.format(username=POSTGRES_USERNAME,password=POSTGRES_PASSWORD,ipaddress=POSTGRES_ADDRESS,port=POSTGRES_PORT,dbname=POSTGRES_DBNAME))\n",
    "# Create the connection\n",
    "team42PS = create_engine(postgres_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the link to panoply, once the create_engine() link has been created, we now have an object that connects to the database. We can now utilize this connection, to run an SQL query to connect to the PostgreSQL database on AWS.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop pre-processing logic on the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the current uncertain economic times, the team made the decision that they wanted to minimize the end-cost of this project. As such, the team wanted to perform as much pre-processing of the data as possible, so that the final cleansed datasets can be easily referenced by the visualization that will be created. However, given the size of this database (64GB) performing cleansing would require a substantial amount of compute power. Instead, the team focused on reviewing a subset of the data, manipulating it in pandas, and then converting the filters to an SQL query so that the data extracted from PostgreSQL was manageable. The subset of data selected is Zone 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pdPostAWS = pd.read_sql_query('''Select * from ais where \\\"Zone\\\" = 20;''', team42PS)\n",
    "\n",
    "pdPostAWS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of generating a dataframe based solely on Zone 20 from the PostgreSQL server took just under 11 minutes on a local SDD drive of 100GB with 16GB of RAM. The size of the dataset that we will review to determine the appropriate SQL query is 512,026 data points. \n",
    "\n",
    "For comparison, this script was re-run on AWS using m5.2xlarge, and the script good 3.09 CPU time, but the Wall Time stayed as 10 minutes and 40 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdPostAWS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Zone 20 Data and Develop a SQL Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note, is that not all of the columns are needed. The only columns needed for the pre-processing task include:\n",
    "* MMSI\n",
    "* BaseDateTime\n",
    "* LAT\n",
    "* LON\n",
    "* VesselType\n",
    "* Status\n",
    "* Length\n",
    "* Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = ['MMSI','BaseDateTime','LAT','LON','VesselType','Status','Length','Width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above python code can be converted to a simple SQL query on a table. The SQL Query would be in the form of a SELECT statement, and it would be: \n",
    "\n",
    "SELECT 'MMSI','BaseDateTime','LAT','LON','VesselType','Status','Length','Width'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonde20_MOD = pdPostAWS.loc[:,cols_to_use]\n",
    "\n",
    "print(zonde20_MOD.shape)\n",
    "\n",
    "zonde20_MOD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reducing the table down to only the columns needed, 9 columns have been removed and the 512,026 rows were kept intact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the extensive amount of \"NaN\" values in this dataset, the team dropped all NaN values. Performing this work on the SQL query will be taxing to the cloud-hosted server, and so this process will be completed once the SQL query has loaded all zones in to the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all data with NaN\n",
    "\n",
    "zonde20_MOD.dropna(inplace=True)\n",
    "\n",
    "print(zonde20_MOD.shape)\n",
    "\n",
    "zonde20_MOD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NaN data cuts the data almost in half, down to 313,397 rows in Zone 20. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team determined that there were only certain statuses that were needed to perfrom the assessment in order to create one row per trip in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out statuses not needed. \n",
    "\n",
    "bad_Status = ['at anchor', 'moored', 'power-driven vessel pushing ahead or towing alongside', 'power-driven vessel towing astern', 'under way using engine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z20_good_Status_DF = zonde20_MOD[zonde20_MOD['Status'].isin(bad_Status)].reset_index(drop=True)\n",
    "\n",
    "print(z20_good_Status_DF.shape)\n",
    "\n",
    "z20_good_Status_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the so-called \"bad statuses\" resulted in an additional 30,000 rows removed from the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the team determined that only certain vessels would be reviewed as part of this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the non-cargo ships\n",
    "z20_assign_vessel = z20_good_Status_DF[((z20_good_Status_DF['VesselType'] >= 70) & (z20_good_Status_DF['VesselType'] <= 89)) | (z20_good_Status_DF['VesselType'].isin({1003,1004,1016,1024}))]\n",
    "\n",
    "z20_grouped_Set = z20_assign_vessel.groupby(\"MMSI\")['Status'].apply(set).reset_index()\n",
    "\n",
    "z_20_groupedMoving = z20_grouped_Set[z20_grouped_Set.apply(lambda x: len(x['Status'])>2, axis=1)]\n",
    "\n",
    "z20_usefulShips = z_20_groupedMoving['MMSI']\n",
    "\n",
    "z20_usefulShips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z20_usefulShips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code provides us with a listing of those ships that provide us with data to leverage. It is now time to re-filter the dataset for only those ships listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_20_useful_Data = z20_assign_vessel[z20_assign_vessel['MMSI'].isin(z20_usefulShips.tolist())].reset_index(drop=True)\n",
    "\n",
    "print(z_20_useful_Data.shape)\n",
    "\n",
    "z_20_useful_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reducing the data down to this list, we now have the smaller dataset that we now need to modify in order to cleanse the data for our visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset the SQL Query to pull data from Zones 16 through 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the steps above, we are going to create a function that takes an integer (Zone) and provides out the cleansed data in the format shown above. This function should be able to run for each zone, and create the table of data cleansed. For testing purposes, we saved our first AWS extract in to an SQLite table, to perform the work on it rather than re-pulling data from AWS each time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usefulShips(df):\n",
    "    chunkVessSet = df.groupby(\"MMSI\")['Status'].apply(set).reset_index()\n",
    "\n",
    "    chunkVessMoving = chunkVessSet[chunkVessSet.apply(lambda x: len(x['Status'])>2, axis=1)]\n",
    "\n",
    "    chuckUsefulShips = chunkVessMoving['MMSI']\n",
    "    \n",
    "    df = df[df['MMSI'].isin(chuckUsefulShips.tolist())].reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "cnxLITE = sqlite3.connect(':memory:')\n",
    "pdPostAWS.to_sql(name='zone20', con=cnxLITE)\n",
    "\n",
    "def cleansedZone_SQLite(zoneNum):\n",
    "    sqlQuery = \"SELECT * FROM zone20 where Zone = \" + str(zoneNum) + \";\"\n",
    "    \n",
    "    zoneDF = pd.DataFrame(columns = cols_to_use)\n",
    "    \n",
    "    for chunk in pd.read_sql_query(sqlQuery , cnxLITE, chunksize=50000):\n",
    "        \n",
    "        chunk = chunk.loc[:,cols_to_use]\n",
    "        \n",
    "        chunk.dropna(inplace=True)\n",
    "\n",
    "        chunk = chunk[chunk['Status'].isin(bad_Status)].reset_index(drop=True)\n",
    "    \n",
    "        chunk = chunk[((chunk['VesselType'] >= 70) & (chunk['VesselType'] <= 89)) | (chunk['VesselType'].isin({1003,1004,1016,1024}))]\n",
    "        \n",
    "        zoneDF = pd.concat([zoneDF,chunk], ignore_index=True)\n",
    "        \n",
    "    \n",
    "    return zoneDF\n",
    "\n",
    "zone20SQLite = usefulShips(cleansedZone_SQLite(20))\n",
    "\n",
    "cnxLITE.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now review the shape and first few rows of our SQLite database extract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zone20SQLite.shape)\n",
    "\n",
    "zone20SQLite.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code proves that our tax worked. As such, we should be able to now modify the code (to pull from AWS versus locally), and get the answer set we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansedZone(zoneNum):\n",
    "    sqlQuery = \"SELECT * FROM ais where \\\"Zone\\\" = \" + str(zoneNum) + \";\"\n",
    "    \n",
    "    zoneDF = pd.DataFrame(columns = cols_to_use)\n",
    "    \n",
    "    for chunk in pd.read_sql_query(sqlQuery , team42PS, chunksize=50000):\n",
    "        \n",
    "        chunk = chunk.loc[:,cols_to_use]\n",
    "        \n",
    "        chunk.dropna(inplace=True)\n",
    "\n",
    "        chunk = chunk[chunk['Status'].isin(bad_Status)].reset_index(drop=True)\n",
    "    \n",
    "        chunk = chunk[((chunk['VesselType'] >= 70) & (chunk['VesselType'] <= 89)) | (chunk['VesselType'].isin({1003,1004,1016,1024}))]\n",
    "        \n",
    "        zoneDF = pd.concat([zoneDF,chunk])\n",
    "        \n",
    "    return zoneDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is where the magic happens. It is recommended that for the below cell, the user utilize a cloud based solution to run, as the datasets average about 10GB per zone (in CSV when they started). However, since the datasets are now in a PostgreSQL server and we are extracting them systematically, it may not take 10GB to download. \n",
    "\n",
    "Personally, when our team ran the below cell on desktop computers with 16GB of RAM, Python exceeded the 16GB of RAM in the computer and crashed. For us, this was our signal that we were starting to deal with \"big data\" and needed to derive a solution that leveraged some sort of framework to leverage enhanced computing power as well as additional RAM. \n",
    "\n",
    "The team will run the below cell on AWS for purposes of demonstrating the end-to-end functionality of the pre-processing workflow. Once the data is cleansed, the team will extract the new dataframe to a CSV file and store it (so that someone can review the end data without the need of an EC2 instance), but the code will still continue to work in the event soemone wants to run on EC2. \n",
    "\n",
    "The team leveraged the guidance on the following medium article regarding how to implement Jupyter on an EC2 system: https://chrisalbon.com/aws/basics/run_project_jupyter_on_amazon_ec2/\n",
    "\n",
    "Based on EC2 specs, and the limitations imposed on AWS Starter Education accounts, the team will implement this code on an m5.2xlarge instance on EC2, which costs (on average) about 38,4 cents per hour. The expectation will be that with the transfer costs 1 cent per GB, the overall costs for pulling the data and extracting it for the Visualizations will cost under \\$5.00. \n",
    "\n",
    "Note that the below cells should NOT be run locally, and should only be run on cloud instances. Our Team has run the below cell to demonstrate our usage of cloud computing strategies, but we also exported the final dataframe to csv, so that it can be loaded locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS EC2 m5.2xlarge Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "listOfZones = [16,17,18,19,20]\n",
    "\n",
    "allZoneData = [cleansedZone(x) for x in listOfZones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allZoneData = usefulShips(pd.concat([x for x in allZoneData]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell took 1 hour 26 minutes and 41 seconds on a m5.2xlarge machine. As such, the team saved this file down for the benefit of the end user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allZoneData.to_csv('CleanFinalDatasetZones20-16.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user is running the script locally, the team recommends utilizing the csv file that was provided in the GitHub repository. This csv file will load just the data necessary. Further in order to leverage multiple processors and speed the process up, the team will load the csv in to a dask dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CleanFinalDatasetZones20-16.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file was run through AWS, please link to \"df\" below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = allZoneData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size the dataframe is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5614782, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>BaseDateTime</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>VesselType</th>\n",
       "      <th>Status</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>367341010</td>\n",
       "      <td>2017-12-01 00:03:24</td>\n",
       "      <td>46.14598</td>\n",
       "      <td>-84.03378</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>192.03</td>\n",
       "      <td>20.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>311017900</td>\n",
       "      <td>2017-12-01 00:01:03</td>\n",
       "      <td>43.48887</td>\n",
       "      <td>-87.53384</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>190.01</td>\n",
       "      <td>23.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>636014410</td>\n",
       "      <td>2017-12-01 00:01:13</td>\n",
       "      <td>28.54872</td>\n",
       "      <td>-88.46215</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>293.20</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367082230</td>\n",
       "      <td>2017-12-01 00:00:23</td>\n",
       "      <td>45.96762</td>\n",
       "      <td>-85.87182</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>moored</td>\n",
       "      <td>221.90</td>\n",
       "      <td>23.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>367127000</td>\n",
       "      <td>2017-12-01 00:01:00</td>\n",
       "      <td>28.61452</td>\n",
       "      <td>-89.63410</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>187.43</td>\n",
       "      <td>27.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MMSI         BaseDateTime       LAT       LON  VesselType  \\\n",
       "0  367341010  2017-12-01 00:03:24  46.14598 -84.03378      1004.0   \n",
       "1  311017900  2017-12-01 00:01:03  43.48887 -87.53384      1004.0   \n",
       "2  636014410  2017-12-01 00:01:13  28.54872 -88.46215      1004.0   \n",
       "3  367082230  2017-12-01 00:00:23  45.96762 -85.87182      1004.0   \n",
       "4  367127000  2017-12-01 00:01:00  28.61452 -89.63410      1024.0   \n",
       "\n",
       "                   Status  Length  Width  \n",
       "0  under way using engine  192.03  20.81  \n",
       "1  under way using engine  190.01  23.60  \n",
       "2  under way using engine  293.20  40.00  \n",
       "3                  moored  221.90  23.79  \n",
       "4  under way using engine  187.43  27.46  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleansed dataset results in 5,614,782 unique nodes to leverage. The important piece to note here is that this is just a node graph, and now we need to convert that to a tibble. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort and Group Ships based on MMSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the specific tibbles, we first need to sort the data so that we can group and create the table correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['MMSI','BaseDateTime'],ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to determine when a ship changes status. We have two columns we like \"MMSI\" and \"Status\". We are looking for when the next row's Status does not equal the current row, so we propose adding one column to our usefulData_Sorted frame that pulls in the next row's status.\n",
    "\n",
    "However, we need to be careful about the following edge case: https://drive.google.com/file/d/168ZbsBsB793YJMRVbc-fGuinRjlItI6u/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6995, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['match_MMSI'] = ~(df.MMSI.eq(df.MMSI.shift()))\n",
    "df['match_Status'] = ~(df.Status.eq(df.Status.shift()))\n",
    "\n",
    "df = df[(df['match_MMSI']) | (df['match_Status'])]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>BaseDateTime</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>VesselType</th>\n",
       "      <th>Status</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>match_MMSI</th>\n",
       "      <th>match_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4015429</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-11 07:23:13</td>\n",
       "      <td>45.59057</td>\n",
       "      <td>-73.50271</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829094</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-12 18:52:24</td>\n",
       "      <td>43.22754</td>\n",
       "      <td>-79.21757</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>moored</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935600</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-14 05:29:15</td>\n",
       "      <td>43.22787</td>\n",
       "      <td>-79.21765</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948403</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-14 09:04:51</td>\n",
       "      <td>43.29329</td>\n",
       "      <td>-79.82692</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>at anchor</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218833</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-18 02:32:51</td>\n",
       "      <td>43.29588</td>\n",
       "      <td>-79.83053</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              MMSI         BaseDateTime       LAT       LON  VesselType  \\\n",
       "4015429  209008000  2017-12-11 07:23:13  45.59057 -73.50271      1004.0   \n",
       "2829094  209008000  2017-12-12 18:52:24  43.22754 -79.21757      1004.0   \n",
       "2935600  209008000  2017-12-14 05:29:15  43.22787 -79.21765      1004.0   \n",
       "2948403  209008000  2017-12-14 09:04:51  43.29329 -79.82692      1004.0   \n",
       "3218833  209008000  2017-12-18 02:32:51  43.29588 -79.83053      1004.0   \n",
       "\n",
       "                         Status  Length  Width  match_MMSI  match_Status  \n",
       "4015429  under way using engine  184.93   23.7        True          True  \n",
       "2829094                  moored  184.93   23.7       False          True  \n",
       "2935600  under way using engine  184.93   23.7       False          True  \n",
       "2948403               at anchor  184.93   23.7       False          True  \n",
       "3218833  under way using engine  184.93   23.7       False          True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above dataframe, we have two issues of note:\n",
    "\n",
    "1. Ships that begin with a status of \"under way using engine\" are one edge we need to consider. This means that the ship is \"en route\"...to somewhere that we don't care about. Thus, we need to ensure that the last row for any MMSI in our table is either \"moored\" or \"at anchor\".\n",
    "2. The corrollary to the first bullet point is a status where it is \"under way using engine\". In order to start a trip, we need to be either \"moored\" or \"at anchor\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>BaseDateTime</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>VesselType</th>\n",
       "      <th>Status</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2829094</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-12 18:52:24</td>\n",
       "      <td>43.22754</td>\n",
       "      <td>-79.21757</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>moored</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948403</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-14 09:04:51</td>\n",
       "      <td>43.29329</td>\n",
       "      <td>-79.82692</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>at anchor</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222890</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-18 03:43:49</td>\n",
       "      <td>43.27275</td>\n",
       "      <td>-79.78643</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>moored</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4779592</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-24 02:49:25</td>\n",
       "      <td>44.94164</td>\n",
       "      <td>-75.04328</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>at anchor</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803852</th>\n",
       "      <td>209008000</td>\n",
       "      <td>2017-12-24 13:00:42</td>\n",
       "      <td>44.98705</td>\n",
       "      <td>-74.78600</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>moored</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              MMSI         BaseDateTime       LAT       LON VesselType  \\\n",
       "2829094  209008000  2017-12-12 18:52:24  43.22754 -79.21757      Cargo   \n",
       "2948403  209008000  2017-12-14 09:04:51  43.29329 -79.82692      Cargo   \n",
       "3222890  209008000  2017-12-18 03:43:49  43.27275 -79.78643      Cargo   \n",
       "4779592  209008000  2017-12-24 02:49:25  44.94164 -75.04328      Cargo   \n",
       "4803852  209008000  2017-12-24 13:00:42  44.98705 -74.78600      Cargo   \n",
       "\n",
       "            Status  Length  Width  \n",
       "2829094     moored  184.93   23.7  \n",
       "2948403  at anchor  184.93   23.7  \n",
       "3222890     moored  184.93   23.7  \n",
       "4779592  at anchor  184.93   23.7  \n",
       "4803852     moored  184.93   23.7  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notMoving_Status = ['moored','at anchor']\n",
    "\n",
    "df = df[df['Status'].isin(notMoving_Status)]\n",
    "\n",
    "def CargoTanker(vessel):\n",
    "    if (vessel >= 70 and vessel <= 79) or (vessel in {1003,1004,1016}):\n",
    "        return 'Cargo'\n",
    "    else:\n",
    "        return 'Tanker'\n",
    "    \n",
    "temp_df = df.copy()\n",
    "temp_df['VesselType'] = temp_df.apply(lambda row: CargoTanker(row['VesselType']), axis=1)\n",
    "\n",
    "badCols = ['match_MMSI','match_Status']\n",
    "\n",
    "finalData = temp_df.drop(columns=badCols)\n",
    "\n",
    "finalData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3324, 8)\n"
     ]
    }
   ],
   "source": [
    "print(finalData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = finalData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the above steps, we have now gotten our test dataset down from 5,00,000+ rows to a manageable 6,995 rows with good data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEU Implementation Aglorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the team has a cleansed dataset, the data needs to be converted in to one tibble per trip. Right now the data is tall and skinny (each row is point of the graph). In order to develop a tibble (short and fat) of the data, each row needs to have the necessary data for one single trip. \n",
    "\n",
    "The model that is being uitilzed is a hub-and-spoke model. Thus, one single trip will consist of a departure from a node, travel to a departure hub, travel to an arrival hub and then arrival to the final spoke. However, right now the code is only implemented at the node level and so the hubs need to be developed.\n",
    "\n",
    "In order to effectively calculate the hubs, the team was determined to use DBSCAN in sklearn.cluster. To do this, a numpy array would be calculated, that would take LON and LAT from the dataframe for use in the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Ports based on proximity of ships anchoring/mooring near eachother\n",
    "#Create a port_reference that maps a port to a Lat,Lon\n",
    "\n",
    "#Original Code\n",
    "'''\n",
    "lon = df['LON'].to_list()\n",
    "lat = df['LAT'].to_list()\n",
    "Coords = np.array([[i,j] for i,j in zip(lon,lat)])'''\n",
    "\n",
    "Coords = np.array(df.loc[:,['LON','LAT']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure the code worked as efficeintly as possible in sklearn, the team created the Coordinate as a numpy array in order to leverage some of the efficiencies of numpy. \n",
    "\n",
    "The main portion of the algorithm that the team wanted to optimize, was to determine what the appropriate epsilon factor in DBSCAN that would result in the lowest carbon output. As such, the code in order to calculate the CO2 emmissions needed to be embedded within functions that could run iteratively over all of the data through various epsilon data points. The next two code cells summarize some of those functions that were generalized. The first cell includes helper functions for the main calculation cell (the detail behind this calculation can be reviewed in the Team's final project paper). The second code block is the actual function code that takeas the node trips dataframe (pulled from AWS above), and determine the hubs to travel to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a ships length and width, calculate the TEU of the ship\n",
    "def CalcTEU(length, width):\n",
    "    area = width * length\n",
    "    return (0.317688908*area) + (0.0000206756025*(area**2)) - 219.2003311\n",
    "\n",
    "#Given the segments, return the total amount of TEU for all ships taking that segment\n",
    "def mapHubTEU(segment, SEG_temp_total):\n",
    "    total = SEG_temp_total[SEG_temp_total['Segment'] == segment]['Individual_TEU'].to_list()[0]\n",
    "    return total\n",
    "\n",
    "#Given the starting lat and lon of 2 points, calculate the \"as a crow flies\" distance\n",
    "def distCalc(start_LAT, start_LON, end_LAT, end_LON): \n",
    "    lat1 = start_LAT\n",
    "    lat2 = end_LAT\n",
    "    lon1 = start_LON\n",
    "    lon2 = end_LON\n",
    "    \n",
    "    dlat = math.radians(lat2-lat1)\n",
    "    dlon = math.radians(lon2-lon1)\n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return 6371 * c\n",
    "\n",
    "#Given the TEU of the individual ship and the TEU of the combined cargo (if applicable), return the total CO2 of the trip\n",
    "def carbonCalc(ton_TEU, CO2_TEU,start_LAT, start_LON, end_LAT, end_LON):\n",
    "    if CO2_TEU < 18000:\n",
    "        CO2 = (-0.00432978571*CO2_TEU) + (0.000000139479467*(CO2_TEU**2)) + 36.15242554 #gCO2/tonne-km\n",
    "    else:\n",
    "        CO2 = 3.1\n",
    "    dist = distCalc(start_LAT, start_LON, end_LAT, end_LON)\n",
    "    tonage = ton_TEU * 15\n",
    "    return CO2 * tonage * dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterPorts(orig_DF, epsil_Cluster):\n",
    "    clustering = DBSCAN(eps=epsil_Cluster, min_samples=1).fit(Coords)\n",
    "\n",
    "    orig_DF['Port_ID'] = clustering.labels_\n",
    "    port_reference = orig_DF.groupby('Port_ID').mean()[['LON','LAT']]\n",
    "\n",
    "    port_reference.columns = ['CENTER_LON','CENTER_LAT']\n",
    "\n",
    "    finalData_withPorts = pd.merge(orig_DF, port_reference, how='left', left_on='Port_ID',right_index=True)\n",
    "\n",
    "    uniqueTrips = finalData_withPorts.copy()\n",
    "    \n",
    "    uniqueTrips.columns = ['MMSI_E',\"BaseDateTime_E\",\"LAT_E\",\"LON_E\",\"VesselType\",\"Status_E\",\"Length\", \"Width\",\"HUBPORT_E\",\"HUBPORT_LON_E\", \"HUBPORT_LAT_E\"]\n",
    "\n",
    "    columnOrder = [\"VesselType\",\"Length\", \"Width\",'MMSI_E',\"BaseDateTime_E\",\"LAT_E\",\"LON_E\",\"Status_E\",\"HUBPORT_E\",\"HUBPORT_LON_E\", \"HUBPORT_LAT_E\"]\n",
    "\n",
    "    uniqueTrips = uniqueTrips[columnOrder]\n",
    "\n",
    "    uniqueTrips['MMSI_S'] = uniqueTrips.MMSI_E.shift()\n",
    "    uniqueTrips['BaseDateTime_S'] = uniqueTrips.BaseDateTime_E.shift()\n",
    "    uniqueTrips['LAT_S'] = uniqueTrips.LAT_E.shift()\n",
    "    uniqueTrips['LON_S'] = uniqueTrips.LON_E.shift()\n",
    "    uniqueTrips['Status_S'] = uniqueTrips.Status_E.shift()\n",
    "    uniqueTrips['HUBPORT_S'] = uniqueTrips.HUBPORT_E.shift()\n",
    "    uniqueTrips['HUBPORT_LON_S'] = uniqueTrips.HUBPORT_LON_E.shift()\n",
    "    uniqueTrips['HUBPORT_LAT_S'] = uniqueTrips.HUBPORT_LAT_E.shift()\n",
    "\n",
    "    uniqueTrips.dropna(inplace=True)\n",
    "\n",
    "    uniqueTrips['Valid'] = uniqueTrips['MMSI_S']==uniqueTrips['MMSI_E']\n",
    "\n",
    "    final_Unique_Trips = uniqueTrips[uniqueTrips['Valid']]\n",
    "\n",
    "    final_Unique_Trips.columns= ['VesselType', 'Length', 'Width', 'MMSI', 'BaseDateTime_TripEnd', 'LAT_SPOKEEndPort','LON_SPOKEEndPort', 'Status_End', 'ENDHUBPORT_PortID', 'ENDHUBPORT_LON', 'ENDHUBPORT_LAT','MMSI_Start', 'BaseDateTime_Start', 'LAT_SPOKEStartPort', 'LON_SPOKEStartPort', 'Status_Start',\n",
    "           'StartHUBPORT_PortID', 'StartHUBPORT_LON', 'StartHUBPORT_LAT', 'Valid']\n",
    "\n",
    "    finalMappedData = final_Unique_Trips.drop(columns=['MMSI_Start','Valid','Status_End','Status_Start'])\n",
    "\n",
    "    orderFinal = ['MMSI', 'VesselType', 'Length', 'Width','BaseDateTime_Start',\n",
    "           'LAT_SPOKEStartPort', 'LON_SPOKEStartPort', 'StartHUBPORT_PortID',\n",
    "           'StartHUBPORT_LON', 'StartHUBPORT_LAT' , 'BaseDateTime_TripEnd',\n",
    "           'LAT_SPOKEEndPort', 'LON_SPOKEEndPort', 'ENDHUBPORT_PortID',\n",
    "           'ENDHUBPORT_LON', 'ENDHUBPORT_LAT']\n",
    "\n",
    "    finalMappedData = finalMappedData[orderFinal]\n",
    "\n",
    "    finalMappedData['Segment'] = finalMappedData.apply(lambda row: (int(row['StartHUBPORT_PortID']),int(row['ENDHUBPORT_PortID'])), axis=1)\n",
    "    finalMappedData['Individual_TEU'] = finalMappedData.apply(lambda row: CalcTEU(row['Length'],row['Width']), axis=1)\n",
    "    \n",
    "    seg_TEU_total = finalMappedData.groupby('Segment').agg({'Individual_TEU':sum}).reset_index()\n",
    "    finalMappedData['Hub_TEU'] = finalMappedData.apply(lambda row: mapHubTEU(row['Segment'],seg_TEU_total), axis=1)\n",
    "    \n",
    "    finalMappedData['CO2_SpokeStart'] = finalMappedData.apply(lambda row: carbonCalc(row['Individual_TEU'], row['Individual_TEU'], row['LAT_SPOKEStartPort'], row['LON_SPOKEStartPort'], row['StartHUBPORT_LAT'], row['StartHUBPORT_LON']), axis=1)\n",
    "    finalMappedData['CO2_SpokeEnd'] = finalMappedData.apply(lambda row: carbonCalc(row['Individual_TEU'], row['Individual_TEU'], row['LAT_SPOKEEndPort'], row['LON_SPOKEEndPort'], row['ENDHUBPORT_LAT'], row['ENDHUBPORT_LON']), axis=1)\n",
    "    finalMappedData['CO2_Hub_Hub'] = finalMappedData.apply(lambda row: carbonCalc(row['Individual_TEU'], row['Hub_TEU'], row['StartHUBPORT_LAT'], row['StartHUBPORT_LON'], row['ENDHUBPORT_LAT'], row['ENDHUBPORT_LON']), axis=1)\n",
    "\n",
    "    return finalMappedData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the bulk of the equations have been built to perform the necessary calculations, we should test running the equations on one epsilon number to see that the results are as expected. We will test on epsilon of 0.1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 s, sys: 3.82 ms, total: 2.25 s\n",
      "Wall time: 2.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "395077734887.82996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epsilon_Clustering_test = 0.1\n",
    "testData = clusterPorts(df, epsilon_Clustering_test)\n",
    "carbon_total = sum([sum(testData['CO2_SpokeStart'].to_list()), sum(testData['CO2_SpokeEnd'].to_list()), sum(testData['CO2_Hub_Hub'].to_list())])\n",
    "carbon_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>VesselType</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>BaseDateTime_Start</th>\n",
       "      <th>LAT_SPOKEStartPort</th>\n",
       "      <th>LON_SPOKEStartPort</th>\n",
       "      <th>StartHUBPORT_PortID</th>\n",
       "      <th>StartHUBPORT_LON</th>\n",
       "      <th>StartHUBPORT_LAT</th>\n",
       "      <th>...</th>\n",
       "      <th>LON_SPOKEEndPort</th>\n",
       "      <th>ENDHUBPORT_PortID</th>\n",
       "      <th>ENDHUBPORT_LON</th>\n",
       "      <th>ENDHUBPORT_LAT</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Individual_TEU</th>\n",
       "      <th>Hub_TEU</th>\n",
       "      <th>CO2_SpokeStart</th>\n",
       "      <th>CO2_SpokeEnd</th>\n",
       "      <th>CO2_Hub_Hub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2948403</th>\n",
       "      <td>209008000</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017-12-12 18:52:24</td>\n",
       "      <td>43.22754</td>\n",
       "      <td>-79.21757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-79.267991</td>\n",
       "      <td>43.235996</td>\n",
       "      <td>...</td>\n",
       "      <td>-79.82692</td>\n",
       "      <td>1</td>\n",
       "      <td>-79.806127</td>\n",
       "      <td>43.284009</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>1570.343393</td>\n",
       "      <td>4854.103055</td>\n",
       "      <td>2.932176e+06</td>\n",
       "      <td>1.381011e+06</td>\n",
       "      <td>1.905052e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222890</th>\n",
       "      <td>209008000</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017-12-14 09:04:51</td>\n",
       "      <td>43.29329</td>\n",
       "      <td>-79.82692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-79.806127</td>\n",
       "      <td>43.284009</td>\n",
       "      <td>...</td>\n",
       "      <td>-79.78643</td>\n",
       "      <td>1</td>\n",
       "      <td>-79.806127</td>\n",
       "      <td>43.284009</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1570.343393</td>\n",
       "      <td>37071.447419</td>\n",
       "      <td>1.381011e+06</td>\n",
       "      <td>1.418112e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4779592</th>\n",
       "      <td>209008000</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017-12-18 03:43:49</td>\n",
       "      <td>43.27275</td>\n",
       "      <td>-79.78643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-79.806127</td>\n",
       "      <td>43.284009</td>\n",
       "      <td>...</td>\n",
       "      <td>-75.04328</td>\n",
       "      <td>2</td>\n",
       "      <td>-75.037810</td>\n",
       "      <td>44.942870</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1570.343393</td>\n",
       "      <td>3320.302314</td>\n",
       "      <td>1.418112e+06</td>\n",
       "      <td>3.159901e+05</td>\n",
       "      <td>2.322491e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803852</th>\n",
       "      <td>209008000</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017-12-24 02:49:25</td>\n",
       "      <td>44.94164</td>\n",
       "      <td>-75.04328</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-75.037810</td>\n",
       "      <td>44.942870</td>\n",
       "      <td>...</td>\n",
       "      <td>-74.78600</td>\n",
       "      <td>3</td>\n",
       "      <td>-74.779583</td>\n",
       "      <td>44.987990</td>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>1570.343393</td>\n",
       "      <td>3320.302314</td>\n",
       "      <td>3.159901e+05</td>\n",
       "      <td>3.604924e+05</td>\n",
       "      <td>1.149182e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4819003</th>\n",
       "      <td>209008000</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>184.93</td>\n",
       "      <td>23.7</td>\n",
       "      <td>2017-12-24 13:00:42</td>\n",
       "      <td>44.98705</td>\n",
       "      <td>-74.78600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-74.779583</td>\n",
       "      <td>44.987990</td>\n",
       "      <td>...</td>\n",
       "      <td>-73.91337</td>\n",
       "      <td>4</td>\n",
       "      <td>-73.957450</td>\n",
       "      <td>45.299436</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>1570.343393</td>\n",
       "      <td>1570.343393</td>\n",
       "      <td>3.604924e+05</td>\n",
       "      <td>3.303609e+06</td>\n",
       "      <td>5.119809e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              MMSI VesselType  Length  Width   BaseDateTime_Start  \\\n",
       "2948403  209008000      Cargo  184.93   23.7  2017-12-12 18:52:24   \n",
       "3222890  209008000      Cargo  184.93   23.7  2017-12-14 09:04:51   \n",
       "4779592  209008000      Cargo  184.93   23.7  2017-12-18 03:43:49   \n",
       "4803852  209008000      Cargo  184.93   23.7  2017-12-24 02:49:25   \n",
       "4819003  209008000      Cargo  184.93   23.7  2017-12-24 13:00:42   \n",
       "\n",
       "         LAT_SPOKEStartPort  LON_SPOKEStartPort  StartHUBPORT_PortID  \\\n",
       "2948403            43.22754           -79.21757                  0.0   \n",
       "3222890            43.29329           -79.82692                  1.0   \n",
       "4779592            43.27275           -79.78643                  1.0   \n",
       "4803852            44.94164           -75.04328                  2.0   \n",
       "4819003            44.98705           -74.78600                  3.0   \n",
       "\n",
       "         StartHUBPORT_LON  StartHUBPORT_LAT  ... LON_SPOKEEndPort  \\\n",
       "2948403        -79.267991         43.235996  ...        -79.82692   \n",
       "3222890        -79.806127         43.284009  ...        -79.78643   \n",
       "4779592        -79.806127         43.284009  ...        -75.04328   \n",
       "4803852        -75.037810         44.942870  ...        -74.78600   \n",
       "4819003        -74.779583         44.987990  ...        -73.91337   \n",
       "\n",
       "         ENDHUBPORT_PortID  ENDHUBPORT_LON  ENDHUBPORT_LAT  Segment  \\\n",
       "2948403                  1      -79.806127       43.284009   (0, 1)   \n",
       "3222890                  1      -79.806127       43.284009   (1, 1)   \n",
       "4779592                  2      -75.037810       44.942870   (1, 2)   \n",
       "4803852                  3      -74.779583       44.987990   (2, 3)   \n",
       "4819003                  4      -73.957450       45.299436   (3, 4)   \n",
       "\n",
       "         Individual_TEU       Hub_TEU  CO2_SpokeStart  CO2_SpokeEnd  \\\n",
       "2948403     1570.343393   4854.103055    2.932176e+06  1.381011e+06   \n",
       "3222890     1570.343393  37071.447419    1.381011e+06  1.418112e+06   \n",
       "4779592     1570.343393   3320.302314    1.418112e+06  3.159901e+05   \n",
       "4803852     1570.343393   3320.302314    3.159901e+05  3.604924e+05   \n",
       "4819003     1570.343393   1570.343393    3.604924e+05  3.303609e+06   \n",
       "\n",
       "          CO2_Hub_Hub  \n",
       "2948403  1.905052e+07  \n",
       "3222890  0.000000e+00  \n",
       "4779592  2.322491e+08  \n",
       "4803852  1.149182e+07  \n",
       "4819003  5.119809e+07  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the clustering algorithm for one iteration at an epsilon of 0.1, it took 3 seconds (on average), and the output appears to be reasonable. As such, we can now leverage these code blocks to process through the various epsion rates to determine the most effective rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_Clustering = [0.0000000001, 0.00001, 0.0001, 0.001, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1,1.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell takes a long time to run, and we ran it to pull down the numbers in to this Jupyter notebook. Note that if you run the next cell, you should expect that it could take two minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 34.7 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "finalMappedData = [clusterPorts(df, x) for x in epsilon_Clustering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCarbonTotal(temp_DF):\n",
    "    return sum([sum(temp_DF['CO2_SpokeStart'].to_list()), sum(temp_DF['CO2_SpokeEnd'].to_list()), sum(temp_DF['CO2_Hub_Hub'].to_list())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayOfCarbon = [calcCarbonTotal(x) for x in finalMappedData]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the various carbon outputs along with their epsilon values, we need to create one final dataframe taht joins the epsilon values and the carbon values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToConvert = {'Epsiolon':epsilon_Clustering,'Total_Carbon':arrayOfCarbon}\n",
    "df_CarbonOutput = pd.DataFrame(dataToConvert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epsiolon</th>\n",
       "      <th>Total_Carbon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>6.084259e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>6.084259e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>6.060637e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>5.747221e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.000000e-03</td>\n",
       "      <td>5.147208e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>4.909102e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.000000e-02</td>\n",
       "      <td>4.588488e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>4.418731e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.000000e-02</td>\n",
       "      <td>4.235782e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>4.084754e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.000000e-02</td>\n",
       "      <td>4.017203e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.000000e-02</td>\n",
       "      <td>4.010993e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.000000e-02</td>\n",
       "      <td>4.008563e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9.000000e-02</td>\n",
       "      <td>3.987745e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>3.950777e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.500000e-01</td>\n",
       "      <td>3.887840e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>3.889729e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.500000e-01</td>\n",
       "      <td>3.846697e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>3.806145e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.500000e-01</td>\n",
       "      <td>3.769822e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.000000e-01</td>\n",
       "      <td>3.875962e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.500000e-01</td>\n",
       "      <td>3.842845e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>3.822889e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>3.900908e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.000000e-01</td>\n",
       "      <td>4.107356e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>4.429996e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>4.491709e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.500000e-01</td>\n",
       "      <td>5.111371e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.746958e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.250000e+00</td>\n",
       "      <td>6.632825e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Epsiolon  Total_Carbon\n",
       "0   1.000000e-10  6.084259e+11\n",
       "1   1.000000e-05  6.084259e+11\n",
       "2   1.000000e-04  6.060637e+11\n",
       "3   1.000000e-03  5.747221e+11\n",
       "4   5.000000e-03  5.147208e+11\n",
       "5   1.000000e-02  4.909102e+11\n",
       "6   2.000000e-02  4.588488e+11\n",
       "7   3.000000e-02  4.418731e+11\n",
       "8   4.000000e-02  4.235782e+11\n",
       "9   5.000000e-02  4.084754e+11\n",
       "10  6.000000e-02  4.017203e+11\n",
       "11  7.000000e-02  4.010993e+11\n",
       "12  8.000000e-02  4.008563e+11\n",
       "13  9.000000e-02  3.987745e+11\n",
       "14  1.000000e-01  3.950777e+11\n",
       "15  1.500000e-01  3.887840e+11\n",
       "16  2.000000e-01  3.889729e+11\n",
       "17  2.500000e-01  3.846697e+11\n",
       "18  3.000000e-01  3.806145e+11\n",
       "19  3.500000e-01  3.769822e+11\n",
       "20  4.000000e-01  3.875962e+11\n",
       "21  4.500000e-01  3.842845e+11\n",
       "22  5.000000e-01  3.822889e+11\n",
       "23  6.000000e-01  3.900908e+11\n",
       "24  7.000000e-01  4.107356e+11\n",
       "25  8.000000e-01  4.429996e+11\n",
       "26  9.000000e-01  4.491709e+11\n",
       "27  9.500000e-01  5.111371e+11\n",
       "28  1.000000e+00  5.746958e+11\n",
       "29  1.250000e+00  6.632825e+11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CarbonOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now completed the pre-processing of the data for our algorithm. The code below will extract the various carbon-trip tabels at the various epsion values. In order to get the data in to our DASH tables, the team extracted each of the carbon outputs to individual csv files for consumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "for i in epsilon_Clustering:\n",
    "    fileName = 'clusteredDF_' + str(i) + '.csv'\n",
    "    finalMappedData[j].to_csv(fileName)\n",
    "    j += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
